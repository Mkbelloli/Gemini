#-*-Python-*-
ENV_CONTEXT = None
EVAL_MODES = ["eval"]
TARGET_Q_CLIPPING = None
RESET_EPISODE_PERIOD = None
ZERO_OBS = True
IMAGES = False
CONTEXT_RANGE_MIN = (-10, -10, -0.5, -1, -1, -1, -1, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3)
CONTEXT_RANGE_MAX = ( 10,  10,  0.5,  1,  1,  1,  1,  0.5,  0.3,  0.5,  0.3,  0.5,  0.3,  0.5,  0.3)
SUBGOAL_DIM = 15
META_EXPLORE_NOISE = 1.0
EXPLORE_NOISE = 1.0

NUM_COLLECT_PER_UPDATE = 1
NUM_COLLECT_PER_META_UPDATE = 10

uvf/negative_distance.summarize = False
uvf/negative_distance.relative_context = True
train_uvf.use_connected_policies = False
train_uvf.I2HRL_enable_flag=True
train_uvf.I2HRL_lp_embedder_class=%EMBEDDING_POLICY_PROCESS_CLASS
train_uvf.I2HRL_lp_embedder_optimizer=@emb/AdamOptimizer()

# Create embedding policy process
EMBEDDING_POLICY_PROCESS_CLASS = @PolicyRepresentationModule
POLICY_EMBEDDING_SIZE = 30
PolicyRepresentationModule.policy_embedding_size = %POLICY_EMBEDDING_SIZE
PolicyRepresentationModule.transition_history = 10
policy_representation_net.num_output_dims = %POLICY_EMBEDDING_SIZE
policy_representation_net.policy_hidden_layers = (500, 200)
imitation_net.num_output_dims=8
imitation_net.imitation_hidden_layers=(100,100)